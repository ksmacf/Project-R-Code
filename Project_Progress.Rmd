---
title: "Project Progress"
author: "Kenji Macfarlane ID:26006480"
date: "22/08/2019"
output: html_document
---

```{r setup, include=FALSE, eval = FALSE, echo = TRUE, tidy = TRUE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, message = FALSE}
# Load necessary libraries
library(tidyverse)
library(latex2exp)
library(gridExtra)
library(kableExtra)
library(dplyr)
library(caret)
library(xgboost)
library(h2o)
library(pROC)
library(ROCit)
library(tourr)
library(V8)
library(sf)
library(concaveman)
library(purrr)
library(spinebil)
library(plotly)
library(varComp)
library(gifski)
library(htmlwidgets)
rm(list = ls(all.names = TRUE))
load("/home/user1/Desktop/Project-Environment_Updated.RData")
```

Create functions for repeated processes.
```{r}
preselection <- function(data){
  data_rmv <- data %>% 
  filter(Muon.PT_1 != "0" | Electron.PT_1 != "0") %>% # select non-zero values
  filter(Muon.PT_1 == 0 | Electron.PT_1 == 0) %>% # select zero values
  filter(Muon.PT_2 == 0, Electron.PT_2 == 0) %>% # Select zero values in second order PTs
  filter(MissingET.MET > 250) %>% # Select events with MET >250 GeV
  filter(Jet.BTag_1 != "0" | Jet.BTag_2 != "0" | 
         Jet.BTag_3 != "0" | Jet.BTag_4 != "0") # Get rid of events that have no b-tagged jets
  return(data_rmv)
}

make_data <- function(data1, data2){ # always put stops in first slot, tops in second slot.
  # Reduce sample size of stops to match background's (top) sample size
  data1_rmv = preselection(data1)
  data2_rmv = preselection(data2)
  data1 = data1 %>% anti_join(data1_rmv[sample(seq(nrow(data2_rmv),nrow(data1_rmv), by = 1)),])
  # Combine the stop and top data for a complete set
  Data = rbind.data.frame(data1, data2)
  Data = Data[sample(nrow(Data)),]
  return(Data)
}

true_partition <- function(data){
  # Partition the given data in order to cross validate later
  split_set = vector("list",2)
  set.seed(sample(1:1000000001, 1, replace=T))
  tr_indx = createDataPartition(data$Signal, p = 2/3)$Resample1
  split_set[[1]] = data[tr_indx, ]
  split_set[[1]] = data_wrangling(split_set[[1]])
  split_set[[2]] = data[-tr_indx, ]
  split_set[[2]] = data_wrangling(split_set[[2]])
  return(split_set) 
}

part_partition <- function(data){
  # create a second training-test set pair for cross validation via splitting (0.75) the training set.
  split_set = vector("list",2)
  set.seed(sample(1:1000000001, 1, replace=T))
  tr_indx = createDataPartition(data$Signal, p = 0.75)$Resample1
  split_set[[1]] = data[tr_indx, ]
  split_set[[2]] = data[-tr_indx, ]
  return(split_set) 
}

xgb_data <- function(true, part){
  # Turn each dataset into a suitable xgb matrix
  xgb_list = vector("list",4)

  # True_split for real model building
  bm_tr = model.matrix(~. + 0, data = true[[1]] %>% select(-Signal))
  tr_xgbM = xgb.DMatrix(data = bm_tr, label = true[[1]]$Signal)
  xgb_list[[1]] = tr_xgbM
  bm_ts = model.matrix(~. + 0, data = true[[2]] %>% select(-Signal))
  ts_xgbM = xgb.DMatrix(data = bm_ts, label = true[[2]]$Signal)
  xgb_list[[2]] = ts_xgbM
    
  # Part_split for cross-validation
  tr_M = model.matrix(~. + 0, data = part[[1]] %>% select(-Signal))
  xgb_tr = xgb.DMatrix(data = tr_M, label = part[[1]]$Signal)
  xgb_list[[3]] = xgb_tr
  ts_M = model.matrix(~. + 0, data = part[[2]] %>% select(-Signal))
  xgb_ts = xgb.DMatrix(data = ts_M, label = part[[2]]$Signal)
  xgb_list[[4]] = xgb_ts
  
  return(xgb_list)
}


data_wrangling <- function(data){
  data = preselection(data)
  data = data %>%
  mutate(
    Lepton.PT = Electron.PT_1 + Muon.PT_1,
    Lepton.Eta = Electron.Eta_1 + Muon.Eta_1,
    Lepton.Phi = Electron.Phi_1 + Muon.Phi_1,
    Lepton.Type = as.numeric(replace(Electron.PT_1, Electron.PT_1 > 0, 1)) + # 1 for Electrons
    as.numeric(replace(Muon.PT_1, Muon.PT_1 > 0, 2)) # 2 for Muons
  ) %>%
  select(-grep(pattern="^Electron|^Muon",colnames(data))) %>%
  # Define b-jets (N=1) and rest as ordinary jets
  mutate(
    BJet.PT = vector("numeric", NROW(data$Signal)),
    BJet.Mass = vector("numeric", NROW(data$Signal)),
    BJet.Phi = vector("numeric", NROW(data$Signal)),
    BJet.Eta = vector("numeric", NROW(data$Signal)),
    BJet.DeltaPhi = vector("numeric", NROW(data$Signal)),
    BJet.DeltaEta = vector("numeric", NROW(data$Signal)),
    Jet1.PT = vector("numeric", NROW(data$Signal)),
    Jet1.Mass = vector("numeric", NROW(data$Signal)),
    Jet1.Phi = vector("numeric", NROW(data$Signal)),
    Jet1.Eta = vector("numeric", NROW(data$Signal)),
    Jet1.DeltaPhi = vector("numeric", NROW(data$Signal)),
    Jet1.DeltaEta = vector("numeric", NROW(data$Signal)),
    Jet2.PT = vector("numeric", NROW(data$Signal)),
    Jet2.Mass = vector("numeric", NROW(data$Signal)),
    Jet2.Phi = vector("numeric", NROW(data$Signal)),
    Jet2.Eta = vector("numeric", NROW(data$Signal)),
    Jet2.DeltaPhi = vector("numeric", NROW(data$Signal)),
    Jet2.DeltaEta = vector("numeric", NROW(data$Signal)),
    Jet3.PT = vector("numeric", NROW(data$Signal)),
    Jet3.Mass = vector("numeric", NROW(data$Signal)),
    Jet3.Phi = vector("numeric", NROW(data$Signal)),
    Jet3.Eta = vector("numeric", NROW(data$Signal)),
    Jet3.DeltaPhi = vector("numeric", NROW(data$Signal)),
    Jet3.DeltaEta = vector("numeric", NROW(data$Signal))
  )
 
  # Loop through so that the highest b-tagged jets are defined as b-jets, otherwise as regular jets
  for (i in 1:nrow(data)) {
    if (data[i,"Jet.BTag_1"] == "1"){
          data[i,"BJet.PT"] = data[i,"Jet.PT_1"]
          data[i,"BJet.Mass"] = data[i,"Jet.Mass_1"]
          data[i,"BJet.Phi"] = data[i,"Jet.Phi_1"]
          data[i,"BJet.Eta"] = data[i,"Jet.Eta_1"]
          data[i,"BJet.DeltaPhi"] = data[i,"Jet.DeltaPhi_1"]
          data[i,"BJet.DeltaEta"] = data[i,"Jet.DeltaEta_1"]
          data[i,"Jet1.PT"] = data[i,"Jet.PT_2"]
          data[i,"Jet1.Mass"] = data[i,"Jet.Mass_2"]
          data[i,"Jet1.Phi"] = data[i,"Jet.Phi_2"]
          data[i,"Jet1.Eta"] = data[i,"Jet.Eta_2"]
          data[i,"Jet1.DeltaPhi"] = data[i,"Jet.DeltaPhi_2"]
          data[i,"Jet1.DeltaEta"] = data[i,"Jet.DeltaEta_2"]
          data[i,"Jet2.PT"] = data[i,"Jet.PT_3"]
          data[i,"Jet2.Mass"] = data[i,"Jet.Mass_3"]
          data[i,"Jet2.Phi"] = data[i,"Jet.Phi_3"]
          data[i,"Jet2.Eta"] = data[i,"Jet.Eta_3"]
          data[i,"Jet2.DeltaPhi"] = data[i,"Jet.DeltaPhi_3"]
          data[i,"Jet2.DeltaEta"] = data[i,"Jet.DeltaEta_3"]
          data[i,"Jet3.PT"] = data[i,"Jet.PT_4"]
          data[i,"Jet3.Mass"] = data[i,"Jet.Mass_4"]
          data[i,"Jet3.Phi"] = data[i,"Jet.Phi_4"]
          data[i,"Jet3.Eta"] = data[i,"Jet.Eta_4"]
          data[i,"Jet3.DeltaPhi"] = data[i,"Jet.DeltaPhi_4"]
          data[i,"Jet3.DeltaEta"] = data[i,"Jet.DeltaEta_4"]      
    } else if (data[i,"Jet.BTag_2"] == "1"){
          data[i,"BJet.PT"] = data[i,"Jet.PT_2"]
          data[i,"BJet.Mass"] = data[i,"Jet.Mass_2"]
          data[i,"BJet.Phi"] = data[i,"Jet.Phi_2"]
          data[i,"BJet.Eta"] = data[i,"Jet.Eta_2"]
          data[i,"BJet.DeltaPhi"] = data[i,"Jet.DeltaPhi_2"]
          data[i,"BJet.DeltaEta"] = data[i,"Jet.DeltaEta_2"]
          data[i,"Jet1.PT"] = data[i,"Jet.PT_1"]
          data[i,"Jet1.Mass"] = data[i,"Jet.Mass_1"]
          data[i,"Jet1.Phi"] = data[i,"Jet.Phi_1"]
          data[i,"Jet1.Eta"] = data[i,"Jet.Eta_1"]
          data[i,"Jet1.DeltaPhi"] = data[i,"Jet.DeltaPhi_1"]
          data[i,"Jet1.DeltaEta"] = data[i,"Jet.DeltaEta_1"]
          data[i,"Jet2.PT"] = data[i,"Jet.PT_3"]
          data[i,"Jet2.Mass"] = data[i,"Jet.Mass_3"]
          data[i,"Jet2.Phi"] = data[i,"Jet.Phi_3"]
          data[i,"Jet2.Eta"] = data[i,"Jet.Eta_3"]
          data[i,"Jet2.DeltaPhi"] = data[i,"Jet.DeltaPhi_3"]
          data[i,"Jet2.DeltaEta"] = data[i,"Jet.DeltaEta_3"]
          data[i,"Jet3.PT"] = data[i,"Jet.PT_4"]
          data[i,"Jet3.Mass"] = data[i,"Jet.Mass_4"]
          data[i,"Jet3.Phi"] = data[i,"Jet.Phi_4"]
          data[i,"Jet3.Eta"] = data[i,"Jet.Eta_4"]
          data[i,"Jet3.DeltaPhi"] = data[i,"Jet.DeltaPhi_4"]
          data[i,"Jet3.DeltaEta"] = data[i,"Jet.DeltaEta_4"]    
    } else if (data[i,"Jet.BTag_3"] == "1"){
          data[i,"BJet.PT"] = data[i,"Jet.PT_3"]
          data[i,"BJet.Mass"] = data[i,"Jet.Mass_3"]
          data[i,"BJet.Phi"] = data[i,"Jet.Phi_3"]
          data[i,"BJet.Eta"] = data[i,"Jet.Eta_3"]
          data[i,"BJet.DeltaPhi"] = data[i,"Jet.DeltaPhi_3"]
          data[i,"BJet.DeltaEta"] = data[i,"Jet.DeltaEta_3"]
          data[i,"Jet1.PT"] = data[i,"Jet.PT_1"]
          data[i,"Jet1.Mass"] = data[i,"Jet.Mass_1"]
          data[i,"Jet1.Phi"] = data[i,"Jet.Phi_1"]
          data[i,"Jet1.Eta"] = data[i,"Jet.Eta_1"]
          data[i,"Jet1.DeltaPhi"] = data[i,"Jet.DeltaPhi_1"]
          data[i,"Jet1.DeltaEta"] = data[i,"Jet.DeltaEta_1"]
          data[i,"Jet2.PT"] = data[i,"Jet.PT_2"]
          data[i,"Jet2.Mass"] = data[i,"Jet.Mass_2"]
          data[i,"Jet2.Phi"] = data[i,"Jet.Phi_2"]
          data[i,"Jet2.Eta"] = data[i,"Jet.Eta_2"]
          data[i,"Jet2.DeltaPhi"] = data[i,"Jet.DeltaPhi_2"]
          data[i,"Jet2.DeltaEta"] = data[i,"Jet.DeltaEta_2"]
          data[i,"Jet3.PT"] = data[i,"Jet.PT_4"]
          data[i,"Jet3.Mass"] = data[i,"Jet.Mass_4"]
          data[i,"Jet3.Phi"] = data[i,"Jet.Phi_4"]
          data[i,"Jet3.Eta"] = data[i,"Jet.Eta_4"]
          data[i,"Jet3.DeltaPhi"] = data[i,"Jet.DeltaPhi_4"]
          data[i,"Jet3.DeltaEta"] = data[i,"Jet.DeltaEta_4"]   
    } else {
          data[i,"BJet.PT"] = data[i,"Jet.PT_4"]
          data[i,"BJet.Mass"] = data[i,"Jet.Mass_4"]
          data[i,"BJet.Phi"] = data[i,"Jet.Phi_4"]
          data[i,"BJet.Eta"] = data[i,"Jet.Eta_4"]
          data[i,"BJet.DeltaPhi"] = data[i,"Jet.DeltaPhi_4"]
          data[i,"BJet.DeltaEta"] = data[i,"Jet.DeltaEta_4"]
          data[i,"Jet1.PT"] = data[i,"Jet.PT_1"]
          data[i,"Jet1.Mass"] = data[i,"Jet.Mass_1"]
          data[i,"Jet1.Phi"] = data[i,"Jet.Phi_1"]
          data[i,"Jet1.Eta"] = data[i,"Jet.Eta_1"]
          data[i,"Jet1.DeltaPhi"] = data[i,"Jet.DeltaPhi_1"]
          data[i,"Jet1.DeltaEta"] = data[i,"Jet.DeltaEta_1"]
          data[i,"Jet2.PT"] = data[i,"Jet.PT_2"]
          data[i,"Jet2.Mass"] = data[i,"Jet.Mass_2"]
          data[i,"Jet2.Phi"] = data[i,"Jet.Phi_2"]
          data[i,"Jet2.Eta"] = data[i,"Jet.Eta_2"]
          data[i,"Jet2.DeltaPhi"] = data[i,"Jet.DeltaPhi_2"]
          data[i,"Jet2.DeltaEta"] = data[i,"Jet.DeltaEta_2"]
          data[i,"Jet3.PT"] = data[i,"Jet.PT_3"]
          data[i,"Jet3.Mass"] = data[i,"Jet.Mass_3"]
          data[i,"Jet3.Phi"] = data[i,"Jet.Phi_3"]
          data[i,"Jet3.Eta"] = data[i,"Jet.Eta_3"]
          data[i,"Jet3.DeltaPhi"] = data[i,"Jet.DeltaPhi_3"]
          data[i,"Jet3.DeltaEta"] = data[i,"Jet.DeltaEta_3"] 
    }
  }
  
  Jets = !names(data) %in% c("Jet.PT_1", "Jet.PT_2", "Jet.PT_3", "Jet.PT_4", 
                           "Jet.Mass_1", "Jet.Mass_2", "Jet.Mass_3", "Jet.Mass_4",
                           "Jet.Phi_1", "Jet.Phi_2", "Jet.Phi_3", "Jet.Phi_4",
                           "Jet.Eta_1", "Jet.Eta_2", "Jet.Eta_3", "Jet.Eta_4",
                           "Jet.DeltaPhi_1", "Jet.DeltaPhi_2", "Jet.DeltaPhi_3", "Jet.DeltaPhi_4",
                           "Jet.DeltaEta_1", "Jet.DeltaEta_2", "Jet.DeltaEta_3", "Jet.DeltaEta_4",
                           "Jet.BTag_1", "Jet.BTag_2", "Jet.BTag_3", "Jet.BTag_4", 
                           "BJet.Mass", "Jet1.Mass", "Jet2.Mass", "Jet3.Mass")
  data = data[,Jets]
  
  data = data %>% 
    mutate(
      BJet.DeltaR = sqrt(data$BJet.DeltaEta^2 + data$BJet.DeltaPhi^2),
      Jet1.DeltaR = sqrt(data$Jet1.DeltaEta^2 + data$Jet1.DeltaPhi^2),
      Jet2.DeltaR = sqrt(data$Jet2.DeltaEta^2 + data$Jet2.DeltaPhi^2),
      Jet3.DeltaR = sqrt(data$Jet3.DeltaEta^2 + data$Jet3.DeltaPhi^2)
    )
  
  irrelevant = !names(data) %in% c("BJet.DeltaEta", "BJet.DeltaPhi", "Jet1.DeltaPhi", "Jet1.DeltaEta",
                                   "Jet2.DeltaPhi", "Jet2.DeltaEta", "Jet3.DeltaPhi", "Jet3.DeltaEta")
  data = data[,irrelevant]
  order = c("MissingET.MET", "MissingET.Eta", "MissingET.Phi", "ScalarHT.HT", 
            "Lepton.PT", "Lepton.Eta", "Lepton.Phi", "Lepton.Type", 
            "BJet.PT", "BJet.Phi", "BJet.Eta", "BJet.DeltaR",
            "Jet1.PT", "Jet1.Phi", "Jet1.Eta", "Jet1.DeltaR",
            "Jet2.PT", "Jet2.Phi", "Jet2.Eta", "Jet2.DeltaR",
            "Jet3.PT", "Jet3.Phi", "Jet3.Eta", "Jet3.DeltaR", "Signal")
  data = data[,order]
  return(data)
}

perform_cv <- function(Parameters, Data){
  Result <- matrix(nrow = NROW(Parameters), ncol = 2) 
  for (i in 1:NROW(Parameters)){
  xgbcv <- 
    xgb.cv(params = Parameters[i,], data = Data[[3]], nrounds = Parameters[i,2],
           nfold = 10, showsd = T, stratified = T, print_every_n = 10, early_stopping_round = 20,
           maximize = F)
  Result[i,1] <- min(xgbcv$evaluation_log$test_logloss_mean)
  Result[i,2] <- xgbcv$best_iteration
  rm(xgbcv)
  }
  return(Result)
}

assign_label <- function(Data, Value){
  Data = Data
  for (i in 1:NROW(Data$Signal_pred)){
    if (Data$Signal_pred[i] > Value){
       Data$Signal_pred[i] = 1
     } else {
       Data$Signal_pred[i] = 0
     }
    }
  return(Data)
}

distribution <- function(Data, Title){
  ggplot(Data, aes(x = Signal_pred, fill = Signal)) +
    geom_histogram(bins = 50, alpha=.5, position="identity") +
    scale_y_continuous(trans = 'log10') +
    labs(y = "Number of Events", x = "Predicted Values", fill = "Signal Type") +
    scale_fill_discrete(labels = c("Background", "Signal")) +
    theme(legend.position = c(0.5, 0.9), legend.direction = "horizontal") +
    ggtitle(TeX(Title))
}

Values <- function(pred, true, data, stopCS){ # data1 = predicted, data2 = true, data3 = combined data
  Vals = vector("list",10)
  
  confMat = confusionMatrix(table(pred, true[[2]]$Signal))
  
  Ns = round((stopCS*137)*(3*confMat$table[2,2]/(nrow(data)-1000000)))
  Nb = round((2500*137)*(3*confMat$table[2,1]/1000000))
  AMS = sqrt(2*((Ns+Nb+10)*log(1+(Ns/(Nb+10)))-Ns))
  total = confMat$table[1,1]+confMat$table[1,2]+confMat$table[2,1]+confMat$table[2,2]
  
  Vals[[1]] = c("TP", confMat$table[2,2]/total)
  Vals[[2]] = c("TN", confMat$table[1,1]/total)
  Vals[[3]] = c("FP" , confMat$table[2,1]/total)
  Vals[[4]] = c("FN", confMat$table[1,2]/total)
  Vals[[5]] = c("Accuracy", confMat$overall[1])
  Vals[[6]] = c("u(Accuracy)", max(confMat$overall[4]-confMat$overall[1], confMat$overall[1]-confMat$overall[3]))
  Vals[[7]] = c("SNR", Ns/Nb)
  Vals[[8]] = c("Nb", Nb)
  Vals[[9]] = c("Ns", Ns)
  Vals[[10]] = c("AMS", AMS)
  
  return(Vals)
}

group_color <- function(variables){
  col_1 = if_else(variables$Signal==0, "black", "blue")
  col_2 = if_else(variables$Signal_pred==0, "black", "blue")
  # Background = black, Signal = blue
  col = cbind(col_1, col_2, col_1)
  col[col[,1]!=col[,2],3] = if_else(col[col[,1]!=col[,2],1]=="black", "green", "red")
  # False positive = Green and False negative = red
  return(col)
}

false_color <- function(variable){
  for (i in 1:NROW(variable)){
  if (variable[i]=="blue"){
    variable[i] = "#d9252533" 
  } else if (variable[i]=="black") {
    variable[i]= "#1a73e833"
  } else if (variable[i]=="green") {
    variable[i] = "blue"
  } else {
    variable[i]== variable[i]
  }
  }
  return(variable)
}
```





## THIS IS TO NOT KEEP PROGRESS, BUT TO PERFORM TESTS ETC ON FROM NOW 22/11/2019 ##






This section of code obtains the full set of data by data wrangling for the signal (stops) and background (tops).

1) Stop benchmark 1 $ \tilde{t}= \text{TeV}, \chi_1^0 = \text{GeV} $. 
```{r, message = F}
# Load the data of interest
topData <- read.delim(file = "/home/user1/Desktop/Root files/ttbarData_true.txt", sep = ",") 
topData$Signal <-  rep(0,nrow(topData))
stopbm1 <- read.delim(file = "/home/user1/Desktop/Root files/benchmark_01.txt", sep = ",") 
stopbm1$Signal <-  rep(1,nrow(stopbm1))

# Split to true training and test set for classification, including the pre-selection criteria (this is met with the delphes card and by choosing ONE lepton and ONLY ONE lepton for the event).

Data_Benchmark01 <- make_data(stopbm1, topData)
true_split1 <- true_partition(Data_Benchmark01)
part_split1 <- part_partition(true_split1[[1]])
rm(stopbm1)
```

2) Stop benchmark 2 $ \tilde{t}= 1,225 \text{TeV}, \chi_1^0 = 400 \text{GeV} $. 
```{r, message = F}
# Load the data of interest
stopbm2 <- read.delim(file = "/home/user1/Desktop/Root files/benchmark_02.txt", sep = ",") 
stopbm2$Signal <-  rep(1,nrow(stopbm2))

Data_Benchmark02 <- make_data(stopbm2, topData)
true_split2 <- true_partition(Data_Benchmark02)
part_split2 <- part_partition(true_split2[[1]])
rm(stopbm2)
```


3) Stop benchmark 3 $ \tilde{t}= 1.25 \text{TeV}, \chi_1^0 = 100 \text{GeV} $. 
```{r, message = F}
# Load the data of interest
stopbm3 <- read.delim(file = "/home/user1/Desktop/Root files/benchmark_03.txt", sep = ",") 
stopbm3$Signal <-  rep(1,nrow(stopbm3))

Data_Benchmark03 <- make_data(stopbm3, topData)
true_split3 <- true_partition(Data_Benchmark03)
part_split3 <- part_partition(true_split3[[1]])
rm(stopbm3)
```

4) Stop benchmark 4 $ \tilde{t}= 750 \text{GeV}, \chi_1^0 = 1 \text{GeV} $
```{r, message = F}
# Load the data of interest
stop <- read.delim(file = "/home/user1/Desktop/Root files/inside_750GeV.txt", sep = ",") 
stop$Signal <-  rep(1,nrow(stop))

Data_Inside <- make_data(stop, topData)
true_split_I <- true_partition(Data_Inside)
part_split_I <- part_partition(true_split_I[[1]])
rm(stop)
```

Using the h2o package, we can can split the data set into training and test sets
```{r, message=FALSE}
# Run AutoML to find the possibly best model for this
h2o.init()
DD_h2oVer <- as.h2o(Data_benchmark01)
n_seed <- sample(1:1000000001, 1, replace=T)
h_split <- h2o.splitFrame(DD_h2oVer, ratios = 0.75, seed = n_seed)
h_train <- h_split[[1]] # 75% for modelling
h_test <- h_split[[2]]
features <- setdiff(colnames(Data_benchmark01), "Signal")
```

Within the h2o package, there is a function called "h2o.automl" which trains many models in order to obtain the best possible classifier.
Number 1), with a k-fold cross-validation of k=5 and testing 20 models... 
```{r}
automl1 = h2o.automl(x = features,
                    y = "Signal",
                    training_frame = h_train,
                    nfolds = 5,                     # 5-fold Cross-Validation
                    max_models = 20,                # Max number of models
                    stopping_metric = "RMSE",       # Metric to optimize
                    project_name = "automl_stop_search", # Specify a name so you can add more models later
                    seed = n_seed) 
check1 <- automl1@leaderboard
check1
```
Produced a result where a Stacking Ensemble was said to be best.

Number 2), with a k-fold cross-validation of k=10 and testing 100 models... 
```{r}
automl2 = h2o.automl(x = features,
                    y = "Signal",
                    training_frame = h_train,
                    nfolds = 10,                     # 5-fold Cross-Validation
                    max_models = 100,                # Max number of models
                    stopping_metric = "RMSE",       # Metric to optimize
                    project_name = "automl_stop_search", # Specify a name so you can add more models later
                    seed = n_seed) 
check2 <- automl2@leaderboard
check2
```

Number 3), with a k-fold cross-validation of k=10 and testing 200 models... 
```{r}
automl3 = h2o.automl(x = features,
                    y = "Signal",
                    training_frame = h_train,
                    nfolds = 10,                     # 5-fold Cross-Validation
                    max_models = 200,                # Max number of models
                    stopping_metric = "RMSE",       # Metric to optimize
                    project_name = "automl_stop_search", # Specify a name so you can add more models later
                    seed = n_seed) 
check3 <- automl3@leaderboard
check3
```


Build an XGBoost model according to automl (Ensemble methods are not to be considered).
Need to re-run this after re-opening a session in order to run xgboost stuff
```{r}
xgb_bm1 <- xgb_data(true_split1, part_split1)
# 1 is true_train, 2 is true_test, 3 is part_train, 4 is part_test
xgb_bm2 <- xgb_data(true_split2, part_split2)
# 1 is true_train, 2 is true_test, 3 is part_train, 4 is part_test
xgb_bm3 <- xgb_data(true_split3, part_split3)
# 1 is true_train, 2 is true_test, 3 is part_train, 4 is part_test
xgb_bm_I <- xgb_data(true_split_I, part_split_I)
# 1 is true_train, 2 is true_test, 3 is part_train, 4 is part_test

```

Testing with the first benchmark set.
```{r}
tr_test1 <- xgb.train(booster = "gbtree", objective = "binary:logistic", data = xgb_bm1[[1]], nrounds = 500, max_depth = 6, eval_metric = "logloss", n_estimators = 500, gamma = 0, alpha = 0)

pred_test1 <- predict(tr_test1, xgb_bm1[[2]])

for (i in 1:NROW(pred_test1)){
  if (pred_test1[i] > 0.6){
       pred_test1[i] = 1
     } else {
       pred_test1[i] = 0
     }
}

confusionMatrix(table(pred_test1, true_split1[[2]]$Signal))
```

Find the most optimal parameter (This is working fine now. Takes a few hours to run 60 parameters)
```{r message=False, warning=TRUE}
param <- list(booster = "gbtree", objective = "binary:logistic")

new_param <- expand.grid(
  n_estimators = 500,
  nrounds = 500,
  eta = seq(from = 0.1, to = 0.4, by = 0.1), # first try said 0.2 is a godo number, consistent with the h2o version
  max_depth = 5, # first try said 5, consistent with the h2o version
  eval_metric = "logloss",
  alpha = 10:20 # first try said 1 is a good number, so test values closer to it -> turns out 10 was also a good number :/ and then 20... then 17 when checking between 10 and 20
)

new_param <- new_param %>%
  mutate(!!! param)
```

Using the h2o version to see if it doesn't crash...
```{r}
h2o.init()

true_split1[[1]][,"Signal"] <- as.factor(true_split1[[1]][,"Signal"])
true_split1[[2]][,"Signal"] <- as.factor(true_split1[[2]][,"Signal"])
part_split1[[1]][,"Signal"] <- as.factor(part_split1[[1]][,"Signal"])
part_split1[[2]][,"Signal"] <- as.factor(part_split1[[2]][,"Signal"])

true_tr <- as.h2o(true_split1[[1]], destination_frame = "true_train") 
true_ts <- as.h2o(true_split1[[2]], destination_frame = "true_test")
part_tr <- as.h2o(part_split1[[1]], destination_frame = "part_train")
part_ts <- as.h2o(part_split1[[2]], destination_frame = "part_test")
  

h2o.xgb_param <- list(
  ntrees = 500, # 500 seemed to be the best from previous run
  max_depth = 5:7, # 5 showed to be the best from previous run
  col_sample_rate = 1.0, # found to be 1.0 from choices of 0.5, 0.75 and 1.0
  learn_rate = seq(from = 0.1, to = 0.3, by = 0.1),
  stopping_metric = "logloss", # logloss was found to be (by majority) the best
  min_split_improvement = seq(from = 0, to = 0.2, by = 0.05) # gamma = 0 seemed to be the best
  #alpha = c(0, 1e-5, 1e-2, 0.1, 1, 100)
)

grid <- h2o.grid("gbm", 
                 x = 1:40, 
                 y = "Signal",
                 grid_id = "grid",
                 training_frame = true_tr,
                 validation_frame = part_ts,
                 nfolds = 10,
                 distribution = "bernoulli",
                 hyper_params = h2o.xgb_param)

grid_results <- h2o.getGrid(grid_id = "grid", sort_by = "auc", decreasing = TRUE)
best_model <- h2o.getModel(grid_results@model_ids[[1]])

```

A test performace shows that RMSE is the best choice as a stopping metric without any other parameters tuned. 
Will  this change when others are tuned? - logloss seemed to perform better when other parameters are tuned.

```{r}
# Perform xgboost with the h2o version
xgb_h2o <- h2o.xgboost(y = "Signal",
                       x = best_model@allparameters$x,
                       training_frame = true_tr,
                       validation_frame = part_ts,
                       model_id = "model",
                       ntrees = best_model@allparameters$ntrees,
                       learn_rate = best_model@allparameters$learn_rate,
                       max_depth = best_model@allparameters$max_depth,
                       quiet_mode = TRUE,
                       stopping_rounds = 5,
                       distribution = "bernoulli",
                       score_tree_interval = 1,
                       score_each_iteration = TRUE,
                       nfolds = 10,
                       stopping_metric = best_model@allparameters$stopping_metric,
                       fold_assignment = "AUTO",
                       fold_column = NULL,
                       ignore_const_cols = TRUE,
                       offset_column = NULL,
                       weights_column = NULL,
                       keep_cross_validation_models = TRUE,
                       keep_cross_validation_predictions = FALSE,
                       keep_cross_validation_fold_assignment = FALSE,
                       tree_method = "hist",
                       grow_policy = "lossguide",
                       booster = "gbtree",
                       stopping_tolerance = best_model@allparameters$stopping_tolerance, 
                       max_runtime_secs = 0,
                       export_checkpoints_dir = NULL,
                       min_rows = 1, 
                       min_child_weight = 1,
                       sample_rate = 1, 
                       subsample = 1, 
                       col_sample_rate = best_model@allparameters$col_sample_rate,
                       colsample_bylevel = 1,
                       col_sample_rate_per_tree = 1,
                       colsample_bytree = 1, 
                       max_abs_leafnode_pred = 0,
                       max_delta_step = 0, 
                       monotone_constraints = NULL, 
                       min_split_improvement = best_model@allparameters$min_split_improvement,
                       nthread = -1, 
                       max_bins = 256, 
                       max_leaves = 0,  
                       min_sum_hessian_in_leaf = 100, 
                       min_data_in_leaf = 0,
                       rate_drop = 0, 
                       one_drop = FALSE, 
                       skip_drop = 0,
                       reg_lambda = 1, 
                       reg_alpha = best_model@allparameters$reg_alpha,
                       dmatrix_type = "auto", 
                       backend = "auto",
                       gpu_id = 0, 
                       verbose = FALSE,
                       tweedie_power = 1.5,
                       categorical_encoding = "AUTO"
                       )


Preds <- as.data.frame(h2o.predict(xgb_h2o[[1]], true_ts))

confusionMatrix(table(Preds$predict, true_test[1:length(Preds$predict),37]))

```


Create ROC curves based on the test models.
```{r}
xgbpred1 <- predict(tr_xgb1, xgb_bm1[[2]])
xgbpred2 <- predict(tr_xgb2, xgb_bm2[[2]])
xgbpred3 <- predict(tr_xgb3, xgb_bm3[[2]])
xgbpred_I <- predict(tr_xgb_I, xgb_bm_I[[2]])
par(pty = "s")
roc(true_split1[[2]]$Signal, xgbpred1, 
    plot = TRUE, smoothed = TRUE, legacy.axes = TRUE, percent = TRUE, 
    xlab = "False Positive Rate (FPR)", ylab = "True Positive Rate (TPR)", col = "blue3",
    ci = TRUE, ci.alpha = 0.9, stratified = FALSE,
    auc = TRUE, grid=TRUE, print.auc=TRUE, print.auc.x = 85, print.auc.y = 70, show.thres=TRUE)
plot.roc(true_split2[[2]]$Signal, xgbpred2, ci = TRUE, ci.alpha = 0.9,
         percent = TRUE, print.auc = TRUE, add = TRUE, col = "green4", print.auc.x = 85, print.auc.y = 60)
plot.roc(true_split3[[2]]$Signal, xgbpred3, ci = TRUE, ci.alpha = 0.9,
         percent = TRUE, print.auc = TRUE, add = TRUE, col = "darkmagenta", print.auc.x = 85, print.auc.y = 50)
plot.roc(true_split_I[[2]]$Signal, xgbpred_I, ci = TRUE, ci.alpha = 0.9,
         percent = TRUE, print.auc = TRUE, add = TRUE, col = "black", print.auc.x = 85, print.auc.y = 40)
legend("bottomright", legend=c("Benchmark 1", "Benchmark 2", "Benchmark 3", "Benchmark 4"), col = c("blue3", "green4", "darkmagenta", "black"), pch = 15)

```

```{r}
xgb.ggplot.importance(xgb.importance(model = tr_xgb1)) + ggplot2::ylab("Gain") + ggplot2::ggtitle(TeX("Feature Importance for Benchmark1: $m_{\\tilde{t}}=1.2$ TeV and $m_{\\tilde{\\chi}_1^0}=600$ GeV")) + ggplot2::theme(legend.position = c(0.8, 0.5)) 
xgb.ggplot.importance(xgb.importance(model = tr_xgb2)) + ggplot2::ylab("Gain") + ggplot2::ggtitle(TeX("Feature Importance for Benchmark2: $m_{\\tilde{t}}=1.225$ TeV and $m_{\\tilde{\\chi}_1^0}=400$ GeV")) + ggplot2::theme(legend.position = c(0.8, 0.5))
xgb.ggplot.importance(xgb.importance(model = tr_xgb3)) + ggplot2::ylab("Gain") + ggplot2::ggtitle(TeX("Feature Importance for Benchmark3: $m_{\\tilde{t}}=1.25$ TeV and $m_{\\tilde{\\chi}_1^0}=100$ GeV")) + ggplot2::theme(legend.position = c(0.8, 0.5))
```

Using the spinebil function to see how the index performs
```{r}
m <- list(basisMatrix(5,6,10), basisMatrix(3,4,10))
indexList <- list(ashapetour(cl = col1, alpha = 0.4, hull_var = c("blue")))
indexLabels <- list("alpha")
trace <- getTrace(var1[,-11], m, indexList, indexLabels)
plotTrace(trace)
```


Do a check at $\\tilde{t} = 1.2$ TeV and $ \\chi_1^0 = 300 $ GeV
```{r, message = F}
# Load the data of interest
stop <- read.delim(file = "/home/user1/Desktop/Root files/check.txt", sep = ",") 
stop$Signal <-  rep(1,nrow(stop))

Data_check <- make_data(stop, topData)
true_split_check <- true_partition(Data_check)
part_split_check <- part_partition(true_split_check[[1]])
rm(stop)

xgb_check <- xgb_data(true_split_check, part_split_check)
# 1 is true_train, 2 is true_test, 3 is part_train, 4 is part_test

tr_test_check <- xgb.train(booster = "gbtree", objective = "binary:logistic", data = xgb_check[[1]], nrounds = 500, max_depth = 6, eval_metric = "logloss", n_estimators = 500, gamma = 0, alpha = 0)
pred_test_check <- predict(tr_test_check, xgb_check[[2]])

for (i in 1:NROW(pred_test_check)){
  if (pred_test_check[i] > 0.6){
       pred_test_check[i] = 1
     } else {
       pred_test_check[i] = 0
     }
}

      
confusionMatrix(table(pred_test_check, true_split_check[[2]]$Signal))
# Had about 0.94 for initial try
```

```{r, message = False}
xgbcv_result_check <- perform_cv(new_param, xgb_check)
# row/list number of the best xgbcv
best_xgbcv <- which.min(xgbcv_result_check[,1])
best_param <- new_param[best_xgbcv,]

# train on partitioned set
tr_test_check <- xgb.train(params = best_param, data = xgb_check[[3]], nrounds = xgbcv_result_check[best_xgbcv,2])
pred_test_check <- predict(tr_test_check, xgb_check[[4]])

# Do a check to see what is a good cut-off
check <- part_split_check[[2]] %>% mutate(Signal_pred = pred_test_check, Signal = as.factor(Signal))
distribution(check, "Distribution of predicted events with $\\tilde{t} = 1.2$ TeV and $ \\chi_1^0 = 300 $ GeV")
check <- assign_label(check, 0.6)
confusionMatrix(table(check$Signal_pred, check$Signal)) # Showed 0.9494 accuracy
rm(check)

# train on original training set
tr_xgb_check <- xgb.train(params = best_param, 
          data = xgb_check[[1]], nrounds = xgbcv_result_check[best_xgbcv,2])

# test error after prediction
xgbpred_check <- predict(tr_xgb_I, xgb_bm_I[[2]])

data_check <- true_split_check[[2]] %>% mutate(Signal_pred = tr_xgb_check, Signal = as.factor(Signal))
distribution(data_check, "Distribution of predicted events with $\\tilde{t} = 1.2$ TeV and $ \\chi_1^0 = 300 $ GeV")

# Create Confusion Matrix based on the best possible cut-off seen in the plot (roughly 100 Events or less with Backgrounds is good I think)
data_check <- assign_label(data_check, 0.6)

confusionMatrix(table(data_check$Signal_pred, data_check$Signal)) # Showed an improvement to 0.9503

vals_check <- Values(data_check$Signal_pred, true_split_check, Data_check, 0.579584)
```

```{r}
var_check <- true_split_check[[2]] %>% mutate(Signal_pred = xgbpred_check) %>% select("MissingET.MET", "MissingET.Phi", "Lepton.PT", "Lepton.PT", "Lepton.Phi", "ScalarHT.HT", "Signal", "Signal_pred") %>% sample_n(NROW(true_split_check[[2]])/10)
var_check <- assign_label(var_check, 0.6)
#write_csv(var1, "/home/user1/Desktop/var1.csv")
col_check <- group_color(var_check)   
var_check[col_check[,1]!=col_check[,2],"Signal_pred"] <- if_else(col_check[col_check[,1]!=col_check[,2],1]=="black", 2, 3)  
# False positive = Green/2 and False negative = red/3
var_check <- apply(var_check[,-7], 2, scale)

# Save tour history
set.seed(1000)
t_check <- save_history(var_check[order(var_check$Signal_pred),-7], guided_tour(ashapetour(cl = col_check[,3], alpha = 0.4, hull_var = c("black")), search_f = tourr:::search_better, alpha = pi/2, cooling = 0.5), step_size = Inf)

X11()
animate_xy(var_check[,-7], planned_tour(t_check), col = col_check[,3], fps = 50)
#dev.copy2pdf(file = "/home/user1/Desktop/Images for thesis/tourr/test.pdf") # is supposed to copy the plot in pdf
X11()

# Save the tour plot
render(var_check[,-7], planned_tour(t_check), display_xy(col = col_check[,3]), frames = 50, "png", "/home/user1/Desktop/Images for thesis/tourr/check_1/check_1-%03d.png")
# pdf version actually captures the whole path
```

Create a gif
```{r}
all_dir <- list.dirs(path = "/home/user1/Desktop/Images for thesis/tourr/bm1/test", recursive = TRUE)
# change end of file according to the folder I want to create gifs with
for (d in all_dir){
  name <- paste0("/home/user1/Desktop/Images for thesis/tourr", "/bm1_test.gif") # change pasted name to match the run number
  fls <- list.files(d, full.names = TRUE)
  gifski(fls , name, loop = FALSE)
}
```


```{r}
tour_path <- interpolate(f_I, 0.1)
  d <- dim(tour_path)
  var_std <- false_I #[order(as.data.frame(variable)$Signal_pred, decreasing = FALSE),] 
  # Remove the brackets when don't want to order
  var <- read_csv("/home/user1/Desktop/tourr variables/false_I.csv") # enter filename
  
  # Put label on 
  var[, "Signal_pred"] <- if_else(var[, "Signal_pred"]=="2", "FP", "FN")
  
  var <- var %>% select(-Signal) #%>% arrange(Signal_pred)
  mydat <- NULL; myaxes <- NULL
  
  for (i in 1:d[3]) {
    fp <- as.matrix(var_std[,-ncol(var_std)]) %*% matrix(tour_path[,,i], ncol=2)
    fp <- tourr::center(fp)
    entry <- matrix(1:nrow(var_std), ncol = 1)
    fp <- cbind(fp, entry)
    colnames(fp) <- c("d1", "d2", "Event_Number")
    mydat <- rbind(mydat, cbind(fp, rep(i, nrow(fp))))
    fa <- cbind(matrix(0, (ncol(var_std)-1), 2), matrix(tour_path[,,i], ncol=2))
    colnames(fa) <- c("origin1", "origin2", "d1", "d2") 
    myaxes <- rbind(myaxes, cbind(fa, rep(i, nrow(fa))))
  }
  
  colnames(mydat)[4] <- "indx"
  colnames(myaxes)[5] <- "indx"
  df <- as_tibble(mydat) %>% 
    mutate(Prediction = rep(var$Signal_pred, d[3]))
  dfaxes <- as_tibble(myaxes) %>%
    mutate(labels=rep(colnames(var[,-ncol(var)]), d[3]))
  numbers <- round(seq(-0.2, 0.2, 0.4/(ncol(var)-1)), 2) # do this to match dimencions in yloc
  dfaxes_mat <- dfaxes %>%
    mutate(xloc = rep(max(df$d1)+0.4, d[3]*(ncol(var)-1)), yloc=rep(numbers[-4], d[3]), 
           coef=paste(round(dfaxes$d1, 2), ", ", round(dfaxes$d2, 2))) #numbers[-4] due to extra entry
  
  
  p <- ggplot(data = dfaxes, aes(frame = indx)) +
    geom_segment(data=dfaxes, aes(x=d1/2-1.4, xend=origin1-1.4, 
                                  y=d2/2, yend=origin2), colour="grey70") +
    geom_text(data=dfaxes, aes(x=d1/2-1.4, y=d2/2, label=labels), colour="grey70") +
    geom_point(data = df, 
               aes(x = d1, y = d2, colour=Prediction, text=paste("Event Number:", df$Event_Number)), 
               size=1, alpha = 1) + # change alpha to change transparency of points
    scale_colour_manual(values = c("red", "blue")) +
    theme_void() +
    coord_fixed() +
    theme(legend.position="right")
  
  pg <- ggplotly(p) %>%
    animation_opts(150, redraw = FALSE, transition=0) %>%
    layout(legend = list(orientation = "h", xanchor="center", x=0.5, y = 0.01), yaxis = list(showgrid = F, showline = F),
           xaxis = list(scaleanchor = "y", scaleratio = 1, showgrid = F, showline =F))
  pg
```


Save tourr
```{r}
htmlwidgets::saveWidget(as_widget(create_animation(t1, "/home/user1/Desktop/tourr variables/var1.csv", var1)), "/home/user1/Desktop/Images for thesis/plotly animation/bm1.html")

htmlwidgets::saveWidget(as_widget(animation_highlighted(t1, "/home/user1/Desktop/tourr variables/var1.csv", var1)), "/home/user1/Desktop/Images for thesis/plotly animation/bm1_highlighted.html")

htmlwidgets::saveWidget(as_widget(animation_FPFN(f1, "/home/user1/Desktop/tourr variables/false1.csv", false1)), "/home/user1/Desktop/Images for thesis/plotly animation/fpfn1.html")

htmlwidgets::saveWidget(as_widget(animation_signal(sig1, "/home/user1/Desktop/tourr variables/fntp1.csv", fntp1)), "/home/user1/Desktop/Images for thesis/plotly animation/fntp1.html")

htmlwidgets::saveWidget(as_widget(animation_background(back1, "/home/user1/Desktop/tourr variables/fptn1.csv", fptn1)), "/home/user1/Desktop/Images for thesis/plotly animation/fptn1.html")
############
htmlwidgets::saveWidget(as_widget(create_animation(t2, "/home/user1/Desktop/tourr variables/var2.csv", var2)), "/home/user1/Desktop/Images for thesis/plotly animation/bm2.html")

htmlwidgets::saveWidget(as_widget(animation_highlighted(t2, "/home/user1/Desktop/tourr variables/var2.csv", var2)), "/home/user1/Desktop/Images for thesis/plotly animation/bm2_highlighted.html")

htmlwidgets::saveWidget(as_widget(animation_FPFN(f2, "/home/user1/Desktop/tourr variables/false2.csv", false2)), "/home/user1/Desktop/Images for thesis/plotly animation/fpfn2.html")

htmlwidgets::saveWidget(as_widget(animation_signal(sig2, "/home/user1/Desktop/tourr variables/fntp2.csv", fntp2)), "/home/user1/Desktop/Images for thesis/plotly animation/fntp2.html")

htmlwidgets::saveWidget(as_widget(animation_background(back2, "/home/user1/Desktop/tourr variables/fptn2.csv", fptn2)), "/home/user1/Desktop/Images for thesis/plotly animation/fptn2.html")
############
htmlwidgets::saveWidget(as_widget(create_animation(t3, "/home/user1/Desktop/tourr variables/var3.csv", var3)), "/home/user1/Desktop/Images for thesis/plotly animation/bm3.html")

htmlwidgets::saveWidget(as_widget(animation_highlighted(t3, "/home/user1/Desktop/tourr variables/var3.csv", var3)), "/home/user1/Desktop/Images for thesis/plotly animation/bm3_highlighted.html")

htmlwidgets::saveWidget(as_widget(animation_FPFN(f3, "/home/user1/Desktop/tourr variables/false3.csv", false3)), "/home/user1/Desktop/Images for thesis/plotly animation/fpfn3.html")

htmlwidgets::saveWidget(as_widget(animation_signal(sig3, "/home/user1/Desktop/tourr variables/fntp3.csv", fntp3)), "/home/user1/Desktop/Images for thesis/plotly animation/fntp3.html")

htmlwidgets::saveWidget(as_widget(animation_background(back3, "/home/user1/Desktop/tourr variables/fptn3.csv", fptn3)), "/home/user1/Desktop/Images for thesis/plotly animation/fptn3.html")
###########
htmlwidgets::saveWidget(as_widget(create_animation(t_I, "/home/user1/Desktop/tourr variables/var_I.csv", var_I)), "/home/user1/Desktop/Images for thesis/plotly animation/bm_I.html")

htmlwidgets::saveWidget(as_widget(animation_highlighted(t_I, "/home/user1/Desktop/tourr variables/var_I.csv", var_I)), "/home/user1/Desktop/Images for thesis/plotly animation/bm_I_highlighted.html")

htmlwidgets::saveWidget(as_widget(animation_FPFN(f_I, "/home/user1/Desktop/tourr variables/false_I.csv", false_I)), "/home/user1/Desktop/Images for thesis/plotly animation/fpfn_I.html")

htmlwidgets::saveWidget(as_widget(animation_signal(sig_I, "/home/user1/Desktop/tourr variables/fntp_I.csv", fntp_I)), "/home/user1/Desktop/Images for thesis/plotly animation/fntp_I.html")

htmlwidgets::saveWidget(as_widget(animation_background(back_I, "/home/user1/Desktop/tourr variables/fptn_I.csv", fptn_I)), "/home/user1/Desktop/Images for thesis/plotly animation/fptn_I.html")


```


```{r}
Variables <- !names(true_split1[[1]]) %in% c("BJet.Mass", "BJet.DeltaPhi", "BJet.DeltaEta", "Jet1.Mass", "Jet1.DeltaPhi", "Jet1.DeltaEta", "Jet2.Mass", "Jet2.DeltaPhi", "Jet2.DeltaEta", "Jet3.Mass", "Jet3.DeltaPhi", "Jet3.DeltaEta")
true_split <- list(NULL, NULL)
true_split[[1]] <- true_split1[[1]][,Variables]
true_split[[2]] <- true_split1[[2]][,Variables]
part_split <- list(NULL, NULL)
part_split[[1]] <- true_split1[[1]][,Variables]
part_split[[2]] <- true_split1[[2]][,Variables]
xgb_test <- xgb_data(true_split, part_split)

tr_test <- xgb.train(booster = "gbtree", objective = "binary:logistic", data = xgb_test[[1]], nrounds = 500, max_depth = 6, eval_metric = "logloss", n_estimators = 500, gamma = 0, alpha = 0)

pred_test <- predict(tr_test, xgb_test[[2]])

for (i in 1:NROW(pred_test)){
  if (pred_test[i] > 0.75){
       pred_test[i] = 1
     } else {
       pred_test[i] = 0
     }
}

confusionMatrix(table(pred_test, true_split[[2]]$Signal))
xgb.ggplot.importance(xgb.importance(model=tr_test))
```


```{r}

 ggplot(check_data_I, aes(x = Signal_pred, fill = Signal)) +
    geom_histogram(bins = 50, alpha=.5, position="identity") +
    scale_y_continuous(trans = 'log10') +
    labs(y = "Number of Events", x = "Predicted Values", fill = "Signal Type") +
    scale_fill_discrete(labels = c("Background", "Signal")) +
    theme(legend.position = c(0.5, 0.9), legend.direction = "horizontal") 
    #ggtitle(TeX(Title))

```


Get$\Delta\phi(E_T^{Miss},l)$
```{r}
observe3_f[200,]

test <- observe3 %>% mutate(
  ML_Delta.Phi = abs(abs(MissingET.Phi) - abs(Lepton.Phi)),
  Signal = if_else(Signal=="1", "Signal", "Background")
)

ggplot(test, aes(x = ML_Delta.Phi, fill = Signal)) +
  geom_histogram(bins = 50, alpha=.5, position = "identity") +
  #geom_histogram(data = test2, aes(x = ML_Delta.Phi, fill = Signal))+
  labs(y = "Number of Events", x = TeX("$\\Delta\\phi(E_T^{Miss},l)$"), fill = "Signal Type") +
  scale_colour_manual(labels = c("Background-like", "Signal-like")) +
  theme(legend.position = c(0.8, 0.6)) +
  ggtitle(TeX("Distribution of "))



data_phi <- rbind(true_split_I[[1]], true_split_I[[2]]) %>% 
  mutate(
    ML_Delta.Phi = abs(abs(MissingET.Phi) - abs(Lepton.Phi)),
    Signal_Type = if_else(Signal=="1", "Signal", "Background")
  )
data_phi <- data_phi[,Variables]
#test2 <- assign_label(test2, 0.75)
#test2[test2[,"Signal"]!=test2[,"Signal_pred"],"Signal_pred"] <- if_else(test2[test2[,"Signal"]!=test2[,"Signal_pred"],"Signal_pred"]=="1", 2, 3)  

#for (i in 1:NROW(test2)) {
#    if (test2$Signal_pred[i]=="1"){
#      test2$Signal_pred[i] = "True Positive"
#    } else if (test2$Signal_pred[i]=="0") {
#      test2$Signal_pred[i] = "True Negative"
#    } else if (test2$Signal_pred[i]=="2") {
#      test2$Signal_pred[i] = "False Positive"
#    } else {
#      test2$Signal_pred[i] = "False Negative"
#    }
#}

#subset(test2, Signal_pred=="False Positive" | Signal_pred=="False Negative")
ggplot(data_phi, aes(x = ML_Delta.Phi, fill = Signal_Type)) +
  geom_histogram(bins = 50, alpha=.5, position = "identity") +
  #geom_histogram(data = test2, aes(x = ML_Delta.Phi, fill = Signal))+
  labs(y = "Number of Events", x = TeX("$\\Delta\\phi(E_T^{Miss}, l)$"), fill = "Signal Type") +
  #scale_colour_manual(labels = c("Background-like", "Signal-like")) +
  theme(legend.position = c(0.8, 0.6)) +
  ggtitle(TeX("Distribution of $\\Delta\\phi(E_T^{Miss}, l)$ for Benchmark 4: $m_{\\tilde{t}} = 750$ GeV and $m_{\\tilde{\\chi}_1^0} = 1$ GeV"))

#inner_join(as.data.frame(read_csv("/home/user1/Desktop/tourr variables/false_I.csv")), data_phi)
```



```{r}

observe_I <- inner_join(as.data.frame(read_csv("/home/user1/Desktop/tourr variables/var_I.csv")),
                        data_phi)
observe_I_f <- inner_join(as.data.frame(read_csv("/home/user1/Desktop/tourr variables/false_I.csv")),
                          data_phi)

observe3 <- inner_join(as.data.frame(read_csv("/home/user1/Desktop/tourr variables/var3.csv")), 
                       data_phi)
observe3_f <- inner_join(as.data.frame(read_csv("/home/user1/Desktop/tourr variables/false3.csv")),
                         data_phi)

#var[, "Signal_pred"] <- if_else(fuck[, "Signal_pred"]=="2", "FP", "FN")

observe_I_f[,"Signal"]==fuck[,"Signal"]

observe3_f[330,]

observe_I_f[22,]

fuck[533,]
```
